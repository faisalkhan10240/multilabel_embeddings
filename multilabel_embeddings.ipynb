{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b906c5e",
   "metadata": {},
   "source": [
    "# Multi-label Text Classification with PyTorch and Multiple Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2a9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (2.2.6)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (0.21.0)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: kaggle in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from torchtext) (1.17.0)\n",
      "Collecting sentencepiece (from torchtext)\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: bleach in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (6.31.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from kaggle) (0.5.1)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 619.1 kB/s eta 0:00:39\n",
      "    --------------------------------------- 0.5/24.4 MB 619.1 kB/s eta 0:00:39\n",
      "    --------------------------------------- 0.5/24.4 MB 619.1 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 0.8/24.4 MB 521.9 kB/s eta 0:00:46\n",
      "   - -------------------------------------- 0.8/24.4 MB 521.9 kB/s eta 0:00:46\n",
      "   - -------------------------------------- 0.8/24.4 MB 521.9 kB/s eta 0:00:46\n",
      "   - -------------------------------------- 1.0/24.4 MB 470.5 kB/s eta 0:00:50\n",
      "   - -------------------------------------- 1.0/24.4 MB 470.5 kB/s eta 0:00:50\n",
      "   - -------------------------------------- 1.0/24.4 MB 470.5 kB/s eta 0:00:50\n",
      "   -- ------------------------------------- 1.3/24.4 MB 464.4 kB/s eta 0:00:50\n",
      "   -- ------------------------------------- 1.3/24.4 MB 464.4 kB/s eta 0:00:50\n",
      "   -- ------------------------------------- 1.3/24.4 MB 464.4 kB/s eta 0:00:50\n",
      "   -- ------------------------------------- 1.6/24.4 MB 454.6 kB/s eta 0:00:51\n",
      "   -- ------------------------------------- 1.6/24.4 MB 454.6 kB/s eta 0:00:51\n",
      "   -- ------------------------------------- 1.6/24.4 MB 454.6 kB/s eta 0:00:51\n",
      "   --- ------------------------------------ 1.8/24.4 MB 434.2 kB/s eta 0:00:52\n",
      "   --- ------------------------------------ 1.8/24.4 MB 434.2 kB/s eta 0:00:52\n",
      "   --- ------------------------------------ 1.8/24.4 MB 434.2 kB/s eta 0:00:52\n",
      "   --- ------------------------------------ 1.8/24.4 MB 434.2 kB/s eta 0:00:52\n",
      "   --- ------------------------------------ 2.1/24.4 MB 398.7 kB/s eta 0:00:56\n",
      "   --- ------------------------------------ 2.1/24.4 MB 398.7 kB/s eta 0:00:56\n",
      "   --- ------------------------------------ 2.1/24.4 MB 398.7 kB/s eta 0:00:56\n",
      "   --- ------------------------------------ 2.4/24.4 MB 410.9 kB/s eta 0:00:54\n",
      "   --- ------------------------------------ 2.4/24.4 MB 410.9 kB/s eta 0:00:54\n",
      "   --- ------------------------------------ 2.4/24.4 MB 410.9 kB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 412.9 kB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 412.9 kB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 421.4 kB/s eta 0:00:52\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 421.4 kB/s eta 0:00:52\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 431.7 kB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 431.7 kB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 440.2 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 440.2 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 3.9/24.4 MB 484.6 kB/s eta 0:00:43\n",
      "   ------ --------------------------------- 4.2/24.4 MB 508.1 kB/s eta 0:00:40\n",
      "   ------- -------------------------------- 4.7/24.4 MB 555.3 kB/s eta 0:00:36\n",
      "   -------- ------------------------------- 5.0/24.4 MB 577.3 kB/s eta 0:00:34\n",
      "   --------- ------------------------------ 5.5/24.4 MB 620.5 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 5.8/24.4 MB 643.3 kB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 682.5 kB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 722.4 kB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 736.0 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 7.3/24.4 MB 752.6 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 7.9/24.4 MB 787.0 kB/s eta 0:00:22\n",
      "   ------------- -------------------------- 8.1/24.4 MB 798.9 kB/s eta 0:00:21\n",
      "   -------------- ------------------------- 8.7/24.4 MB 832.3 kB/s eta 0:00:19\n",
      "   -------------- ------------------------- 8.9/24.4 MB 837.0 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 9.4/24.4 MB 867.1 kB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 894.2 kB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 903.9 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 929.4 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 11.0/24.4 MB 942.3 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 11.5/24.4 MB 964.8 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 12.1/24.4 MB 990.9 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 12.3/24.4 MB 1.0 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 12.8/24.4 MB 1.0 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.1/24.4 MB 1.0 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.4/24.4 MB 1.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 1.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 1.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 14.9/24.4 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.2/24.4 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.5/24.4 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.7/24.4 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.3/24.4 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 16.8/24.4 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.4/24.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.6/24.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.9/24.4 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.1/24.4 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.1/24.4 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.4/24.4 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 19.7/24.4 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 19.9/24.4 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.2/24.4 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.4/24.4 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.7/24.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.0/24.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.0/24.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.5/24.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.8/24.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.1/24.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.3/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 1.1 MB/s  0:00:22\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.0 MB 1.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.0 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 1.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 1.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.8/12.0 MB 1.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.0 MB 1.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.0 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.7/12.0 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.9/12.0 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.2/12.0 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.5/12.0 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.7/12.0 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.5/12.0 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.5/12.0 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.8/12.0 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.0/12.0 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.0/12.0 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.3/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/12.0 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/12.0 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.4/12.0 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.3/12.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 1.6 MB/s  0:00:07\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 524.3/566.1 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 566.1/566.1 kB 2.0 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 2.7 MB/s  0:00:01\n",
      "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.5/1.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.4 MB/s  0:00:00\n",
      "Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl (60 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml): started\n",
      "  Building wheel for fasttext (pyproject.toml): finished with status 'error'\n",
      "Failed to build fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for fasttext (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [45 lines of output]\n",
      "      C:\\Users\\asus\\AppData\\Local\\Temp\\pip-build-env-7orkfuo8\\overlay\\Lib\\site-packages\\setuptools\\dist.py:599: SetuptoolsDeprecationWarning: Invalid dash-separated key 'description-file' in 'metadata' (setup.cfg), please use the underscore name 'description_file' instead.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "              (Affected: fasttext).\n",
      "      \n",
      "              By 2026-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self._enforce_underscore(opt, section)\n",
      "      C:\\Users\\asus\\AppData\\Local\\Temp\\pip-build-env-7orkfuo8\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: MIT License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-313\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-313\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-313\\fasttext\n",
      "      creating build\\lib.win-amd64-cpython-313\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-313\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-313\\fasttext\\util\n",
      "      creating build\\lib.win-amd64-cpython-313\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-313\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-313\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-313\\fasttext\\tests\n",
      "      running build_ext\n",
      "      building 'fasttext_pybind' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: failed-wheel-build-for-install\n",
      "\n",
      "× Failed to build installable wheels for some pyproject.toml based projects\n",
      "╰─> fasttext\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch torchvision torchtext scikit-learn gensim fasttext sentence-transformers kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101f3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "kaggle_json = {\"username\":\"faisalkhan1024\",\"key\":\"8cffe453c216b8c4dcb334288d39f734\"}\n",
    "with open('/root/.kaggle/kaggle.json','w') as f: json.dump(kaggle_json,f)\n",
    "os.chmod('/root/.kaggle/kaggle.json',600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c371605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/prox37/twitter-multilabel-classification-dataset\n",
      "License(s): other\n",
      "Downloading twitter-multilabel-classification-dataset.zip to c:\\Users\\asus\\Downloads\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.07M [00:00<?, ?B/s]\n",
      "100%|██████████| 1.07M/1.07M [00:00<00:00, 96.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d prox37/twitter-multilabel-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d3be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"twitter-multilabel-classification-dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf6f5a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1296010336907038720t</td>\n",
       "      <td>@cath__kath AstraZeneca is made with the kidne...</td>\n",
       "      <td>ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1336808189677940736t</td>\n",
       "      <td>It begins. Please find safe alternatives to th...</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1329488407307956231t</td>\n",
       "      <td>@PaolaQP1231 Well, I mean congratulations Covi...</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1364194604459900934t</td>\n",
       "      <td>@BorisJohnson for those of us that do not wish...</td>\n",
       "      <td>mandatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1375938799247765515t</td>\n",
       "      <td>She has been trying to speak out: writing lett...</td>\n",
       "      <td>side-effect rushed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                              tweet  \\\n",
       "0  1296010336907038720t  @cath__kath AstraZeneca is made with the kidne...   \n",
       "1  1336808189677940736t  It begins. Please find safe alternatives to th...   \n",
       "2  1329488407307956231t  @PaolaQP1231 Well, I mean congratulations Covi...   \n",
       "3  1364194604459900934t  @BorisJohnson for those of us that do not wish...   \n",
       "4  1375938799247765515t  She has been trying to speak out: writing lett...   \n",
       "\n",
       "               labels  \n",
       "0         ingredients  \n",
       "1         side-effect  \n",
       "2         side-effect  \n",
       "3           mandatory  \n",
       "4  side-effect rushed  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\asus\\Desktop\\multi_label_classification\\mLabel_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c175c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\asus\\Desktop\\multi_label_classification\\mLabel_tweets.csv')\n",
    "\n",
    "# 1. split labels -> multiple classes\n",
    "df['labels'] = df['labels'].apply(lambda x: x.split(','))\n",
    "\n",
    "# 2. Multi-hot encode\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y = mlb.fit_transform(df['labels'])\n",
    "\n",
    "# 3. Text column select\n",
    "X = df['tweet']\n",
    "\n",
    "# 4. Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b3cf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\asus\\Desktop\\multi_label_classification\\mLabel_tweets.csv')\n",
    "\n",
    "# Clean and split labels by comma and space\n",
    "def split_labels(x):\n",
    "    # Remove extra spaces\n",
    "    x = x.strip()\n",
    "    # Identify separators: comma or space\n",
    "    parts = re.split(r'[,\\s]+', x)\n",
    "    return [p for p in parts if p]   # remove empty strings\n",
    "\n",
    "df['labels'] = df['labels'].apply(split_labels)\n",
    "\n",
    "# Multi-hot encoding\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y = mlb.fit_transform(df['labels'])\n",
    "\n",
    "# tweets\n",
    "X = df['tweet']\n",
    "\n",
    "# Train / Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c7e8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (1.15.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Using cached wrapt-2.0.1-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Using cached gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Using cached wrapt-2.0.1-cp313-cp313-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/3 [wrapt]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   ---------------------------------------- 3/3 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3194d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.8 MB/s  0:00:00\n",
      "Using cached regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/3 [regex]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   ---------------------------------------- 3/3 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 nltk-3.9.2 regex-2025.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30a0223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb6aeda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c03fa8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences=[nltk.word_tokenize(t.lower()) for t in X_train]\n",
    "w2v=Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "924b792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_w2v_vector(text):\n",
    "    words=nltk.word_tokenize(text.lower())\n",
    "    vecs=[w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    return np.mean(vecs,axis=0) if vecs else np.zeros(100)\n",
    "\n",
    "X_train_w2v=np.array([get_w2v_vector(t) for t in X_train])\n",
    "X_test_w2v=np.array([get_w2v_vector(t) for t in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3528db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(input_dim,256),nn.ReLU(),nn.Dropout(0.3),\n",
    "            nn.Linear(256,128),nn.ReLU(),\n",
    "            nn.Linear(128,6),nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x): return self.model(x)\n",
    "\n",
    "def train_model(Xtr_emb,Xte_emb):\n",
    "    Xtr=torch.tensor(Xtr_emb,dtype=torch.float32)\n",
    "    Xte=torch.tensor(Xte_emb,dtype=torch.float32)\n",
    "    Ytr=torch.tensor(y_train.values,dtype=torch.float32)\n",
    "    Yte=torch.tensor(y_test.values,dtype=torch.float32)\n",
    "\n",
    "    loader=DataLoader(TensorDataset(Xtr,Ytr),batch_size=32,shuffle=True)\n",
    "    model=Classifier(Xtr_emb.shape[1])\n",
    "    opt=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    loss_fn=nn.BCELoss()\n",
    "\n",
    "    for ep in range(3):\n",
    "        for xb,yb in loader:\n",
    "            opt.zero_grad()\n",
    "            pred=model(xb)\n",
    "            loss=loss_fn(pred,yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print('Epoch',ep,'Loss',loss.item())\n",
    "\n",
    "    return model(Xte).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51df4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "num_labels = y_train.shape[1]\n",
    "print(num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ba2f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_emb, X_test_emb):\n",
    "    Xtr = torch.tensor(X_train_emb, dtype=torch.float32)\n",
    "    Xte = torch.tensor(X_test_emb, dtype=torch.float32)\n",
    "\n",
    "    Ytr = torch.tensor(y_train, dtype=torch.float32)\n",
    "    Yte = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(Xtr, Ytr), batch_size=32, shuffle=True)\n",
    "\n",
    "    num_labels = y_train.shape[1]   # AUTOMATIC OUTPUT SIZE\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train_emb.shape[1], 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, num_labels),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    preds_test = model(Xte).detach().numpy()\n",
    "\n",
    "    return preds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a52c9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD2VEC F1: 0.24463373083475298\n"
     ]
    }
   ],
   "source": [
    "preds_w2v=train_model(X_train_w2v,X_test_w2v)\n",
    "from sklearn.metrics import f1_score\n",
    "print('WORD2VEC F1:',f1_score(y_test,(preds_w2v>0.5),average='micro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01191b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
